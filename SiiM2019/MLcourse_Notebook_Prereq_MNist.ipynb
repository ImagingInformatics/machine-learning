{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Example\n",
        "\n",
        "### Working in jupyter notebooks to build a neural-network with Keras\n",
        "\n",
        "\n",
        "This is a simple quick-start for performing digit recognition with a neural network in Keras.\n",
        "\n",
        "It is largely based on the `mnist_mlp.py` example from the Keras source, as well as:\n",
        "\n",
        "https://github.com/wxs/keras-mnist-tutorial\n",
        "\n",
        "The mnist dataset and information can be found at:\n",
        "\n",
        "http://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "*\"The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
        "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\"*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time to build a neural network!\n",
        "First let's import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q tensorflow==2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_classes = 10\n",
        "\n",
        "# the data, shuffled and split between tran and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(\"X_train original shape\", X_train.shape)\n",
        "print(\"y_train original shape\", y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at some examples of the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Class {}\".format(y_train[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Format the data for training\n",
        "Our neural-network is going to take a single vector for each training example, so we need to reshape the input so that each 28x28 image becomes a single 784 dimensional vector. We'll also scale the inputs to be in the range [0-1] rather than [0-255]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(\"Training matrix shape\", X_train.shape)\n",
        "print(\"Testing matrix shape\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modify the target matrices to be in the one-hot format, i.e.\n",
        "\n",
        "```\n",
        "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "etc.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why perform this \"one-hot\" encoding?\n",
        "\n",
        "A nice explanation is provided at:\n",
        "    \n",
        "https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build the neural network\n",
        "Build the neural-network. Here we'll do a simple 3 layer fully connected network.\n",
        "<img src=\"./figures/mnist_network_figure.png\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(784,)))\n",
        "model.add(Activation('relu')) # An \"activation\" is just a non-linear function applied to the output\n",
        "                              # of the layer above. Here, with a \"rectified linear unit\",\n",
        "                              # we clamp all values below 0 to 0.\n",
        "\n",
        "model.add(Dropout(0.1))   # Dropout helps protect the model from memorizing or\n",
        "model.add(Dense(512))     # \"overfitting\" the training data\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax')) # This special \"softmax\" activation among other things,\n",
        "                                 # ensures the output is a valid probability distribution,\n",
        "                                 # (i.e. that its values are all non-negative and sum to 1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compile the model\n",
        "Keras is built on top of TensorFlow, a package that allows you to define a *computational graph* in Python, which is then compiled and ran efficiently on the CPU or GPU without the overhead of the Python interpreter.\n",
        "\n",
        "When compiing a model, Keras asks you to specify your **loss function** and your **optimizer**. The loss function we'll use here is called *categorical crossentropy*, and is a loss function well-suited to comparing two probability distributions.\n",
        "\n",
        "Here our predictions are probability distributions across the ten different digits (e.g. \"we're 80% confident this image is a 3, 10% sure it's an 8, 5% it's a 2, etc.\"), and the target is a probability distribution with 100% for the correct category, and 0 for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution. [More detail at Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)\n",
        "\n",
        "The optimizer helps determine how quickly the model learns, how resistent it is to getting \"stuck\" or \"blowing up\". We won't discuss this in too much detail, but \"adam\" is often a good choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model!\n",
        "Here you feed the training data loaded in earlier into this model and it will learn to classify digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(X_train, Y_train,\n",
        "          batch_size=128, epochs=8,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(history.history.keys())\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'], 'orange', label='Training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], 'blue', label='Validation accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch', fontsize=16)\n",
        "plt.ylabel('Accuracy', fontsize = 16)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'], 'red', label='Training loss')\n",
        "plt.plot(history.history['val_loss'], 'green', label='Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch', fontsize=16)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finally, evaluate its performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score = model.evaluate(X_test, Y_test,verbose=0)\n",
        "\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inspecting the output\n",
        "\n",
        "It's always a good idea to inspect the output and make sure everything looks correct. Here we'll look at some examples it gets right, and some examples it gets wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The predict_classes function outputs the highest probability class\n",
        "# according to the trained classifier for each input example.\n",
        "predicted_classes = model.predict(X_test)\n",
        "predicted_classes = np.argmax(predicted_classes,axis=1)\n",
        "\n",
        "\n",
        "# Check which items we got right / wrong\n",
        "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
        "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Examples network got correct')\n",
        "plt.figure(figsize=(10,10))\n",
        "for i, correct in enumerate(correct_indices[:9]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
        "plt.show()\n",
        "\n",
        "print('Examples network got incorrect')\n",
        "plt.figure(figsize=(10,10))\n",
        "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Congrats!!!\n",
        "\n",
        "### You have trained a neural network to perform fairly accurate prediction of handwritten digits.\n",
        "\n",
        "Feel free to try different models, as well as change the regularization of the network (i.e. dropout).\n",
        "\n",
        "How do these changes affect training, validation, and testing???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are lots of other great examples at the Keras homepage at http://keras.io and in the source code at https://github.com/fchollet/keras"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
