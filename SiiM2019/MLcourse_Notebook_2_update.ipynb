{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung Classification Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this course you will learn how to run a deep learning experiment - from loading data, running a model, and deploying your model on a test dataset.\n",
    "\n",
    "This course will build upon the knowledge gained in the first lesson and will utilize a much larger dataset.\n",
    "\n",
    "In this course you will build a deep learning model that identifies whether an x-ray of the lungs contains an opacity. The dataset is from a Kaggle challenge.\n",
    "\n",
    "The dataset comes from the RSNA Pneumonia Detection Challenge (Kaggel API)\n",
    "        ,\n",
    "        \"The [Radiological Society of North America](http://www.rsna.org/) Pneumonia Detection Challenge: https://www.kaggle.com/c/rsna-pneumonia-detection-challenge\",\n",
    "\n",
    "\n",
    "<img src=\"figures/lesson2_datasetImage.png\">\n",
    "\n",
    "In this notebook some of the cells need to be entered by you to work on completing the assignment. These cells have:\n",
    "\n",
    "```python\n",
    "#--------EDIT THIS CELL------------\n",
    "```\n",
    "\n",
    "at the top of the cell.\n",
    "\n",
    "For instance, you will see a few cells down where you need to setup the variable 'p' to store the data for this project.\n",
    "\n",
    "```python\n",
    "#--------EDIT THIS CELL------------\n",
    "\n",
    "# Load project data into a variable 'p'\n",
    "```\n",
    "\n",
    "The 'helper_utils.ipynb' file has details that can be used if you get stuck.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the mdai module\n",
    "import mdai\n",
    "mdai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mdai client\n",
    "mdai_client = mdai.Client(domain='public.md.ai', access_token=\"ENTER TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------EDIT THIS CELL------------\n",
    "\n",
    "# Load project data into a variable 'p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.show_label_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------EDIT THIS CELL------------\n",
    "\n",
    "# map the label ids to class ids as a dictionary object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print label dictionary and set up\n",
    "\n",
    "print(labels_dict)\n",
    "p.set_labels_dict(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dataset ID and label mappings\n",
    "p.show_datasets() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display label classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = p.get_dataset_by_id('D_ao3XWQ')\n",
    "dataset.prepare()\n",
    "dataset.show_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = dataset.get_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataset into train, val, and test\n",
    "\n",
    "train_dataset, val_dataset = mdai.common_utils.train_test_split(dataset, validation_split = 0.98)\n",
    "val_dataset, test_dataset = mdai.common_utils.train_test_split(val_dataset, validation_split = 0.995)\n",
    "test_dataset, test_dataset2 = mdai.common_utils.train_test_split(test_dataset, validation_split = 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = dataset.get_annotations(labels_dict.keys(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_ids = train_dataset.get_image_ids()\n",
    "val_image_ids = val_dataset.get_image_ids()\n",
    "\n",
    "# visualize a few train images \n",
    "mdai.visualize.display_images(train_image_ids[:2], cols=2)\n",
    "mdai.visualize.display_images(val_image_ids[:2], cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example extracting pixel array from the dicom data\n",
    "import numpy as np\n",
    "# get image pixel data\n",
    "pixel_array = mdai.visualize.load_dicom_image(train_image_ids[0], to_RGB=False, rescale=True)\n",
    "print(np.shape(pixel_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keras module\n",
    "from keras import applications\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Dense, GlobalAveragePooling2D, Conv2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters \n",
    "img_width = 128\n",
    "img_height = 128\n",
    "epochs = 20\n",
    "\n",
    "params = {\n",
    "    'dim': (img_width, img_height),\n",
    "    'batch_size': 8,\n",
    "    'n_classes': 2,\n",
    "    'n_channels': 3,\n",
    "    'shuffle': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Defining Model\n",
    "\n",
    "Here we build up a very basic CNN architecture (similar in nature to the VGG class of architectures).\n",
    "\n",
    "Here is where you can feel free to experiment with different architectures and tune the hyperparameters of the network. You should observe differences in training performance, as well as the amount of time required to fully train the network. \n",
    "\n",
    "Try changing the number of kernels in the network from 32 down to 16.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "conv1 = Conv2D(16, (3,3), activation = 'relu', padding='same')(inputs)\n",
    "```\n",
    "\n",
    "Or changing the size of the filter kernels from 3x3 to 5x5\n",
    "\n",
    "```python\n",
    "conv1 = Conv2D(32, (5,5), activation = 'relu', padding='same')(inputs)\n",
    "```\n",
    "\n",
    "Or the activation function for the output:\n",
    "\n",
    "```python\n",
    "conv1 = Conv2D(32, (3,3), activation = 'tanh', padding='same')(inputs)\n",
    "```\n",
    "\n",
    "How do these parameters affect performance and training time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------EDIT THIS CELL------------\n",
    "\n",
    "# Create a CNN model to train \n",
    "# This can be similar to the one used in the previous notebook \n",
    "# (i.e. chest vs. abdomen X-ray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdai.utils import keras_utils\n",
    "\n",
    "train_generator = keras_utils.DataGenerator(train_dataset, **params)\n",
    "val_generator = keras_utils.DataGenerator(val_dataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=4, verbose=2),\n",
    "    ModelCheckpoint(filepath='best_model_lesson2.h5', monitor='val_acc', \n",
    "                    save_best_only=True, verbose=2)\n",
    "]\n",
    "\n",
    "history = model.fit_generator(\n",
    "            generator=train_generator,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,            \n",
    "            validation_data=val_generator,\n",
    "            use_multiprocessing=True, \n",
    "            workers=8)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------EDIT THIS CELL------------\n",
    "\n",
    "# Write code to plot learning curves \n",
    "# for both training and validation (accuracy and loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model_lesson2.h5')\n",
    "test_dataset.prepare()\n",
    "print(len(test_dataset.image_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from skimage.transform import resize\n",
    "from PIL import Image \n",
    "\n",
    "for image_id in test_dataset.image_ids[80:100]: \n",
    "    \n",
    "    image = mdai.visualize.load_dicom_image(image_id, to_RGB=True)\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize((img_width, img_height))\n",
    "    \n",
    "    x = np.expand_dims(image, axis=0)    \n",
    "    y_prob = model.predict(x) \n",
    "    y_classes = y_prob.argmax(axis=-1)\n",
    "    \n",
    "    title = 'Pred: ' + test_dataset.class_id_to_class_text(y_classes[0]) + ', Prob:' + str(round(y_prob[0][y_classes[0]], 3))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success!!!\n",
    "\n",
    "Feel free to continue working further on in this notebook to:\n",
    "\n",
    "- Develop new models\n",
    "- Work on ways to evaluate models\n",
    "- etc. etc. etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
