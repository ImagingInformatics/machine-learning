{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chest/Abdomen Classification Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### In this course you will learn how to run a deep learning experiment - from loading data, running a model, and deploying your model on a test dataset.\n",
        "\n",
        "This course is based off of the lesson found at:\n",
        "https://colab.research.google.com/github/mdai/ml-lessons/blob/master/lesson1-xray-images-classification.ipynb\n",
        "\n",
        "The dataset is composed of Training (n=65), Validation (n=10), and Test (n=2). The dataset has been annotated corresponding to the anatomy covered within the X-ray (i.e. chest or abdomen). Therefore, each image is related to a corresponding class/label.\n",
        "\n",
        "The goal of the exercise is to develop a deep learning model that can learn to automatically classify the images as either being x-rays of the chest or the abdomen.\n",
        "\n",
        "<img src=\"images/lesson1_datasetImage.png\">\n",
        "\n",
        "This updated notebook includes the original notebook provided in the course materials provided before the couse, as well as an updated section exploring a transfer learning based approach, as well as activation maps to visualize what the network focused on to make the classification decision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Include the mdai module, output current mdai version\n",
        "!pip install -q --upgrade mdai\n",
        "!pip install -q tensorflow==2.12.0\n",
        "import mdai\n",
        "mdai.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add mdai client\n",
        "mdai_client = mdai.Client(domain='public.md.ai', access_token=\"ENTER TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pull down data\n",
        "\n",
        "After authenticating to MD.AI - you are now able to download available datasets. You can visit:\n",
        "\n",
        "https://public.md.ai/hub/projects/public\n",
        "\n",
        "To see the datasets that are available.\n",
        "\n",
        "We are now going to pull down the data locally to your computer.\n",
        "\n",
        "##### Note - on a Windows machine the Dicom files may cause an issue due to the length of file name.\n",
        "\n",
        "In this case you can change the path to store at a location closer to root\n",
        "\n",
        "For example change path to:\n",
        "\n",
        "path = 'C:/Users/Public/chest-abd-data'\n",
        "\n",
        "Thus code will look like:\n",
        "\n",
        "```python\n",
        "p = mdai_client.project('PVq9raBJ', path='C:/Users/Public/chest-abd-data')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull down dataset. Save into specified path (e.g., ./chest-abd-data)\n",
        "p = mdai_client.project('PVq9raBJ', path='./chest-abd-data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the corresponding label groups for the abdomen and chest\n",
        "p.show_label_groups()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### We will now map the label ids to the corresponding class ids in the form of a dictionary object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The following maps label ids to class ids as a dict obj\n",
        "labels_dict = {'L_38Y7Jl':0, # Abdomen\n",
        "               'L_z8xEkB':1, # Chest\n",
        "              }\n",
        "\n",
        "print(labels_dict)\n",
        "p.set_labels_dict(labels_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show dataset ID and label mappings\n",
        "p.show_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create training dataset\n",
        "train_dataset = p.get_dataset_by_name('TRAIN')\n",
        "train_dataset.prepare()\n",
        "train_image_ids = train_dataset.get_image_ids()\n",
        "print('The number of images in the training dataset is:')\n",
        "print(len(train_image_ids)) # display the number of images\n",
        "print('')\n",
        "\n",
        "# create the validation dataset\n",
        "val_dataset = p.get_dataset_by_name('VAL')\n",
        "val_dataset.prepare()\n",
        "val_image_ids = val_dataset.get_image_ids()\n",
        "print('The number of images in the validation dataset is:')\n",
        "print(len(val_image_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize a few training images\n",
        "mdai.visualize.display_images(train_image_ids[:2], cols=2)\n",
        "mdai.visualize.display_images(val_image_ids[:2], cols=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Image files are a range of different sizes, the following shows how to read an example image from the training set and print out the size of the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# get image pixel data\n",
        "pixel_array = mdai.visualize.load_dicom_image(train_image_ids[0], to_RGB=False, rescale=False)\n",
        "print('The size of the image is:')\n",
        "print(np.shape(pixel_array))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import keras modules and begin defining model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import keras module\n",
        "from keras import applications\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dropout, Flatten, Dense, GlobalAveragePooling2D, Conv2D\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define model parameters\n",
        "img_width = 192\n",
        "img_height = 192\n",
        "epochs = 20\n",
        "\n",
        "params = {\n",
        "    'dim': (img_width, img_height),\n",
        "    'batch_size': 4,\n",
        "    'n_classes': 2,\n",
        "    'n_channels': 3,\n",
        "    'shuffle': True,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Begin Defining Model\n",
        "\n",
        "Here we build up a very basic CNN architecture (similar in nature to the VGG class of architectures).\n",
        "\n",
        "Here is where you can feel free to experiment with different architectures and tune the hyperparameters of the network. You should observe differences in training performance, as well as the amount of time required to fully train the network.\n",
        "\n",
        "Try changing the number of kernels in the network from 32 down to 16.\n",
        "\n",
        "For example:\n",
        "\n",
        "```python\n",
        "conv1 = Conv2D(16, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "```\n",
        "\n",
        "Or changing the size of the filter kernels from 3x3 to 5x5\n",
        "\n",
        "```python\n",
        "conv1 = Conv2D(32, (5,5), activation = 'relu', padding='same')(inputs)\n",
        "```\n",
        "\n",
        "Or the activation function for the output:\n",
        "\n",
        "```python\n",
        "conv1 = Conv2D(32, (3,3), activation = 'tanh', padding='same')(inputs)\n",
        "```\n",
        "\n",
        "How do these parameters affect performance and training time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a basic CNN architecture\n",
        "\n",
        "inputs = Input((img_width, img_height, 3))\n",
        "\n",
        "# Block1\n",
        "conv1 = Conv2D(32, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "conv2 = Conv2D(32, (3,3), activation = 'relu', padding='same')(conv1)\n",
        "max1 = MaxPooling2D((4,4), strides = (4,4))(conv2)\n",
        "\n",
        "# Block2\n",
        "conv3 = Conv2D(64, (3,3), activation = 'relu', padding='same')(max1)\n",
        "conv4 = Conv2D(64, (3,3), activation = 'relu', padding='same')(conv3)\n",
        "max2 = MaxPooling2D((4,4), strides = (4,4))(conv4)\n",
        "\n",
        "fcn = Flatten()(max2)\n",
        "fcn = Dense(256,activation='relu')(fcn)\n",
        "fcn = Dropout(0.2)(fcn)\n",
        "fcn = Dense(128,activation='relu')(fcn)\n",
        "fcn = Dropout(0.1)(fcn)\n",
        "output = Dense(2,activation='softmax')(fcn)\n",
        "\n",
        "model = Model(inputs,output)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=1.E-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
        "              loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End Defining Model\n",
        "\n",
        "Now we will set up the data generators that will be used during training of the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mdai.utils import keras_utils\n",
        "\n",
        "train_generator = keras_utils.DataGenerator(train_dataset, **params)\n",
        "val_generator = keras_utils.DataGenerator(val_dataset, **params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set callback functions to early stop training and save the best model so far\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, verbose=2),\n",
        "    ModelCheckpoint(filepath='best_model.h5', monitor='val_loss',\n",
        "                    save_best_only=True, verbose=2)\n",
        "]\n",
        "\n",
        "history = model.fit_generator(\n",
        "            generator=train_generator,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1, #on a Windows machine you may want to use verbose=0\n",
        "            validation_data=val_generator,\n",
        "            use_multiprocessing=True,\n",
        "            workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ----------------------- Finish Training -----------------------\n",
        "\n",
        "#### Now our network is trained. We can generate learning curves that visualize how the model performed on the training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#print(history.history.keys())\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'], 'orange', label='Training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], 'blue', label='Validation accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch', fontsize=16)\n",
        "plt.ylabel('Accuracy', fontsize = 16)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'], 'red', label='Training loss')\n",
        "plt.plot(history.history['val_loss'], 'green', label='Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch', fontsize=16)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Studying learning curves gives insight into how the network is performing and assess properties such as whether overfitting is an issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create the Test dataset\n",
        "\n",
        "In this example we have an example image from each class (i.e. one abdominal and one chest X-ray). Let's find out how our model performs on this hold out test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_weights('best_model.h5')\n",
        "test_dataset = p.get_dataset_by_name('TEST')\n",
        "test_dataset.prepare()\n",
        "print(len(test_dataset.image_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Make a prediction on the test images as to whether they are a chest X-ray or if they are an abdominal X-ray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#from skimage.transform import resize\n",
        "from PIL import Image\n",
        "\n",
        "for image_id in test_dataset.image_ids:\n",
        "\n",
        "    image = mdai.visualize.load_dicom_image(image_id, to_RGB=True)\n",
        "    image = Image.fromarray(image)\n",
        "    image = image.resize((img_width, img_height))\n",
        "\n",
        "    x = np.expand_dims(image, axis=0)\n",
        "    y_prob = model.predict(x)\n",
        "    y_classes = y_prob.argmax(axis=-1)\n",
        "\n",
        "    title = 'Pred: ' + test_dataset.class_id_to_class_text(y_classes[0]) + ', Prob:' + str(round(y_prob[0][y_classes[0]], 3))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Congratulations, you have just run a deep learning experiment!\n",
        "\n",
        "The experiment used a CNN architecture to learn to distinguish abdominal X-rays from chest X-rays.\n",
        "\n",
        "Continue experimenting with the code contained in this notebook. We will start the course with a discussion of important concepts related to machine/deep learning. We will then work through this notebook, as well as others that will be downloaded during the course.\n",
        "\n",
        "Now is the time to ask yourself:\n",
        "\n",
        "- How did your model perform?\n",
        "\n",
        "- Did it correctly identify whether an X-ray was from the chest or abdomen?\n",
        "\n",
        "- What other network parameters could you experiment with to try to further improve performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ------------ Transfer Learning Slides -----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's work through a transfer learning type example with the same dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = applications.mobilenet.MobileNet(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "model_top  = Sequential()\n",
        "model_top.add(GlobalAveragePooling2D(input_shape=base_model.output_shape[1:], data_format=None))\n",
        "model_top.add(Dense(256, activation='relu'))\n",
        "model_top.add(Dropout(0.5))\n",
        "model_top.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=model_top(base_model.output))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08,decay=0.0),\n",
        "              loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set callback functions to early stop training and save the best model so far\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, verbose=2),\n",
        "    ModelCheckpoint(filepath='best_model_tl.h5', monitor='val_loss',\n",
        "                    save_best_only=True, verbose=2)\n",
        "]\n",
        "\n",
        "history = model.fit_generator(\n",
        "            generator=train_generator,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1, #on a Windows machine you may want to use verbose=0\n",
        "            validation_data=val_generator,\n",
        "            use_multiprocessing=True,\n",
        "            workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# print(history.history.keys())\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'], 'orange', label='Training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], 'blue', label='Validation accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch', fontsize=16)\n",
        "plt.ylabel('Accuracy', fontsize = 16)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'], 'red', label='Training loss')\n",
        "plt.plot(history.history['val_loss'], 'green', label='Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch', fontsize=16)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test model on the hold out set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#from skimage.transform import resize\n",
        "from PIL import Image\n",
        "\n",
        "for image_id in test_dataset.image_ids:\n",
        "\n",
        "    image = mdai.visualize.load_dicom_image(image_id, to_RGB=True)\n",
        "    image = Image.fromarray(image)\n",
        "    image = image.resize((img_width, img_height))\n",
        "\n",
        "    x = np.expand_dims(image, axis=0)\n",
        "    y_prob = model.predict(x)\n",
        "    y_classes = y_prob.argmax(axis=-1)\n",
        "\n",
        "    title = 'Pred: ' + test_dataset.class_id_to_class_text(y_classes[0]) + ', Prob:' + str(round(y_prob[0][y_classes[0]], 3))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ------------ Activation Maps Slides ---------------\n",
        "\n",
        "Another way the networks can be further explored is by characterizing the regions in an image that the network focuses on to make a decision. For this we will work on generating activation maps.\n",
        "\n",
        "For most activation map approaches, networks have to be designed with some specific components that allow us to probe specific layers and generate which regions in an image are causing the most activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
        "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
        "from keras import backend as K\n",
        "import h5py\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "def global_average_pooling(x):\n",
        "    return K.mean(x, axis = (2,3))\n",
        "\n",
        "def global_average_pooling_shape(input_shape):\n",
        "    return input_shape[0:2]\n",
        "\n",
        "def VGG_like_convolutions():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(ZeroPadding2D((1,1),input_shape=(192,192,3)))\n",
        "    model.add(Convolution2D(16, (3, 3), activation='relu', name='conv1_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(16, (3, 3), activation='relu', name='conv1_2'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(32, (3, 3), activation='relu', name='conv2_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(32, (3, 3), activation='relu', name='conv2_2'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(64, (3, 3), activation='relu', name='conv3_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(64, (3, 3), activation='relu', name='conv3_2'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(128, (3, 3), activation='relu', name='conv4_1'))\n",
        "    model.add(ZeroPadding2D((1, 1)))\n",
        "    model.add(Convolution2D(128, (3, 3), activation='relu', name='conv4_2'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_model_vgg_like():\n",
        "    model = VGG_like_convolutions()\n",
        "\n",
        "    model.add(Lambda(global_average_pooling,\n",
        "              output_shape=global_average_pooling_shape))\n",
        "    model.add(Dense(2, activation = 'softmax', kernel_initializer='uniform'))\n",
        "    sgd = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def get_output_layer(model, layer_name):\n",
        "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "    layer = layer_dict[layer_name]\n",
        "    return layer\n",
        "\n",
        "model = get_model_vgg_like()\n",
        "\n",
        "# Set callback functions to early stop training and save the best model so far\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, verbose=2),\n",
        "    ModelCheckpoint(filepath='best_model_am2.h5', monitor='val_loss',\n",
        "                    save_best_only=True, verbose=2)\n",
        "]\n",
        "\n",
        "history = model.fit_generator(\n",
        "            generator=train_generator,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1, #on a Windows machine you may want to use verbose=0\n",
        "            validation_data=val_generator,\n",
        "            use_multiprocessing=True,\n",
        "            workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def get_output_layer(model, layer_name):\n",
        "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "    layer = layer_dict[layer_name]\n",
        "    return layer\n",
        "\n",
        "def visualize_class_activation_map(model, img):\n",
        "    model = model\n",
        "    original_img = img\n",
        "    width = 192\n",
        "    height = 192\n",
        "\n",
        "    #Get the 512 input weights to the softmax.\n",
        "    class_weights = model.layers[-1].get_weights()[0]\n",
        "    final_conv_layer = get_output_layer(model, \"conv4_2\")\n",
        "    get_output = K.function([model.layers[0].input], \\\n",
        "        [final_conv_layer.output,\n",
        "    model.layers[-1].output])\n",
        "    [conv_outputs, predictions] = get_output([np.float32(img)])\n",
        "\n",
        "    conv_outputs = conv_outputs[0, :, :, :]\n",
        "\n",
        "    #Create the class activation map.\n",
        "    cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[0:2])\n",
        "    for i, w in enumerate(class_weights[:, 1]):\n",
        "            cam += w * conv_outputs[:, :, i]\n",
        "    #print(\"predictions\", predictions)\n",
        "\n",
        "    cam = cv2.resize(cam, (height, width))\n",
        "\n",
        "    return cam\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize the activation maps overlaid on the test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "for image_id in test_dataset.image_ids:\n",
        "\n",
        "    image = mdai.visualize.load_dicom_image(image_id, to_RGB=True)\n",
        "    image = Image.fromarray(image)\n",
        "    image = image.resize((img_width, img_height))\n",
        "\n",
        "    x = np.expand_dims(image, axis=0)\n",
        "    y_prob = model.predict(x)\n",
        "    y_classes = y_prob.argmax(axis=-1)\n",
        "\n",
        "    title = 'Pred: ' + test_dataset.class_id_to_class_text(y_classes[0]) + ', Prob:' + str(round(y_prob[0][y_classes[0]], 3))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.imshow(image, cmap='jet')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Add the activation map overlay\n",
        "    cam = visualize_class_activation_map(model, x)\n",
        "    cam = cam - np.min(cam)\n",
        "    cam[cam<np.max(cam)/2]=0\n",
        "    masked = np.ma.masked_where(cam == 0, cam)\n",
        "    plt.imshow(masked,cmap='jet',alpha=0.3)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate learning curves for this final network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(history.history.keys())\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'], 'orange', label='Training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], 'blue', label='Validation accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch', fontsize=16)\n",
        "plt.ylabel('Accuracy', fontsize = 16)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'], 'red', label='Training loss')\n",
        "plt.plot(history.history['val_loss'], 'green', label='Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch', fontsize=16)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Great work!!!\n",
        "\n",
        "Feel free to experiment with any of the cells in the spreadsheet. Particularly testing out different network architectures and trying to optimize hyperparameters as well.\n",
        "\n",
        "One way to do this is to first copy this notebook and try making changes there. This way you keep a clean original copy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
