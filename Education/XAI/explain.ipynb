{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TS80GaFrWPO"
      },
      "source": [
        "# Explaining the Predictions of a Convolutional Neural Network on Head CT Images\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Sulam-Group/h-shap/blob/jaco/siim-ml-tutorial/demo/RSNA_ICH_detection/explain.ipynb)\n",
        "\n",
        "## Task\n",
        "\n",
        "Explain the predictions of a Convolutional Neural Network (CNN) trained to predict the presence of intracranial hemorrhage on the [RSNA 2019 Brain CT Hemorrhage Challenge dataset](https://www.kaggle.com/competitions/rsna-intracranial-hemorrhage-detection/data).\n",
        "\n",
        "## Requirements\n",
        "\n",
        "1. Basic understanding of machine learning and deep learning.\n",
        "2. Programming in Python.\n",
        "\n",
        "## Learning objectives\n",
        "\n",
        "1. Load a pre-trained classifier in PyTorch.\n",
        "2. Use the classifier to predict the presence of hemorrhage in test images.\n",
        "3. Explain the predictions of the classifier to detect intracranial hemorrhage.\n",
        "\n",
        "## Acknowledgements\n",
        "\n",
        "This Jupyter Notebook was based on code by Jacopo Teneggi ([jtenegg1@jhu.edu](mailto:jtenegg1@jhu.edu))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnvAuzXRrWPU"
      },
      "source": [
        "## Load the pretrained model\n",
        "\n",
        "Here, we load a pretrained model which was trained on the [RSNA 2019 Brain CT Hemorrhage Challenge dataset](https://www.kaggle.com/competitions/rsna-intracranial-hemorrhage-detection/data). The model is trained on the binary classification problem of predicting `1` when an images contains any type of hemorrhage, or `0` when the image is healthy.\n",
        "\n",
        "This step requires PyTorch. Follow the instructions at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) to install PyTorch. Note that, although not required, this Jupyter Notebook supports execution on a GPU. When not running on GPU, expect long runtimes.\n",
        "\n",
        "If running on [Google Colab](https://colab.research.google.com/), you can enable GPU runtime by selecting `Runtime > Change runtime type > GPU` in the `Hardware accelerator` dropdown menu. When running on GPU on a free Google Colab account, explaining model predictions should take around 50 seconds per image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7AgSyKrrWPW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms.functional as tf\n",
        "\n",
        "# Define the device to run on\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the pretrained model\n",
        "torch.set_grad_enabled(False)\n",
        "model = torch.hub.load(\n",
        "    \"Sulam-Group/h-shap:jaco/siim-ml-tutorial\", \"rsnahemorrhagenet\", trust_repo=\"check\"\n",
        ")\n",
        "model = model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr-aKafZrWPX"
      },
      "source": [
        "## Use the Model to Predict on Images\n",
        "\n",
        "Here, we use the pretrained model to predict on 4 positive test images from the [CQ500 dataset](http://headctstudy.qure.ai/dataset). \n",
        "\n",
        "Ground-truth annotations of the bleeds are provided by the [BHX extension](https://physionet.org/content/bhx-brain-bounding-box/1.1/) dataset, and are highlighted with red solid lines in the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QysxjFcErWPY"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import json\n",
        "import requests as req\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def download(url: str) -> io.BytesIO:\n",
        "    \"\"\"\n",
        "    A helper function to download an object from a url.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    url: str\n",
        "        The url to download the object from.\n",
        "    \"\"\"\n",
        "    res = req.get(url)\n",
        "    res.raise_for_status()\n",
        "    return io.BytesIO(res.content)\n",
        "\n",
        "\n",
        "def window(img: np.ndarray, WL: int, WW: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    A function that windows the values of an image at level\n",
        "    `WL` with width `WW` (in Hounsfield Units).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    img: np.ndarray\n",
        "        The image to window.\n",
        "    WL: int\n",
        "        The window level.\n",
        "    WW: int\n",
        "        The window width.\n",
        "    \"\"\"\n",
        "    image_min = WL - WW // 2\n",
        "    image_max = WL + WW // 2\n",
        "    img[img < image_min] = image_min\n",
        "    img[img > image_max] = image_max\n",
        "\n",
        "    img = (img - image_min) / (image_max - image_min)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def annotate(annotation_df: pd.DataFrame, ax: plt.Axes) -> None:\n",
        "    \"\"\"\n",
        "    A helper function to annotate an image with its ground-truth\n",
        "    annotations.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    annotation_df: pd.DataFrame\n",
        "        The DataFrame containing the annotations.\n",
        "    ax: plt.Axes\n",
        "        The Axes containing the image to annotate.\n",
        "    \"\"\"\n",
        "    for _, annotation_row in annotation_df.iterrows():\n",
        "        annotation = annotation_row[\"data\"].replace(\"'\", '\"')\n",
        "        annotation = json.loads(annotation)\n",
        "        bbox_x = annotation[\"x\"]\n",
        "        bbox_y = annotation[\"y\"]\n",
        "        bbox_width = annotation[\"width\"]\n",
        "        bbox_height = annotation[\"height\"]\n",
        "\n",
        "        bbox = patches.Rectangle(\n",
        "            (bbox_x, bbox_y),\n",
        "            bbox_width,\n",
        "            bbox_height,\n",
        "            linewidth=1,\n",
        "            edgecolor=\"r\",\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        ax.add_patch(bbox)\n",
        "\n",
        "\n",
        "# Download images and ground truth annotations;\n",
        "# images are windowed with the standard brain\n",
        "# setting, i.e. WW = 80 and WL = 40 .\n",
        "base_url = \"https://github.com/Sulam-Group/h-shap/blob/jaco/siim-ml-tutorial/demo/RSNA_ICH_detection/images\"\n",
        "sop_ids = [\n",
        "    \"1.2.276.0.7230010.3.1.4.296485376.1.1521713007.1700822\",\n",
        "    \"1.2.276.0.7230010.3.1.4.296485376.1.1521713021.1704518\",\n",
        "    \"1.2.276.0.7230010.3.1.4.296485376.1.1521713469.1816851\",\n",
        "    \"1.2.276.0.7230010.3.1.4.296485376.1.1521713940.1946656\",\n",
        "]\n",
        "images = np.stack(\n",
        "    [\n",
        "        window(\n",
        "            np.load(download(f\"{base_url}/{sop_id}.npy?raw=true\")).astype(np.float32),\n",
        "            WL=40,\n",
        "            WW=80,\n",
        "        )\n",
        "        for sop_id in tqdm(sop_ids)\n",
        "    ]\n",
        ")\n",
        "gt = pd.read_csv(f\"{base_url}/gt.csv?raw=true\")\n",
        "\n",
        "# Visualize images and ground truth annotations,\n",
        "# predict the presence of hemorrhage\n",
        "# in each image using the pretrained model.\n",
        "_, axes = plt.subplots(1, len(images), figsize=(16, 9))\n",
        "for i in range(len(images)):\n",
        "    # Preprocess image for prediction\n",
        "    x = images[i]\n",
        "    x = tf.to_tensor(x)\n",
        "    x = x.unsqueeze(1).repeat(1, 3, 1, 1)\n",
        "    x = tf.normalize(x, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    # Use the model to predict the presence of hemorrhage\n",
        "    x = x.to(device)\n",
        "    output = model(x)\n",
        "    prediction = (output > 0.5).long()\n",
        "\n",
        "    # Visualize image and ground truth annotations\n",
        "    ax = axes[i]\n",
        "    ax.imshow(images[i], cmap=\"gray\")\n",
        "\n",
        "    annotation_df = gt[gt[\"SOPInstanceUID\"] == sop_ids[i]]\n",
        "    annotate(annotation_df, ax)\n",
        "\n",
        "    ax.set_title(f\"Prediction: {prediction.item()}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5o3f2RlrWPa"
      },
      "source": [
        "## Explain Model Predictions Using `h-Shap`, `PartitionExplainer`, and `Grad-CAM`\n",
        "\n",
        "In the context on machine learning, explanations are used to highlight those features in an input that contributed the most towards a model prediction. For images, typically, explanation methods produce **saliency map**---heatmaps where the intensity of every pixel represents their importance. Here, we will showcase how to produce saliency maps using `h-Shap`, `PartitionExplainer`, and `Grad-CAM`:\n",
        "\n",
        "* [`h-shap`](https://ieeexplore.ieee.org/document/9826424) is a Python package that provides an exact, fast, and hierarchical implmentation of the [Shapley value](https://en.wikipedia.org/wiki/Shapley_value). Source code is available on [GitHub](https://github.com/Sulam-Group/h-shap).\n",
        "\n",
        "* `PartitionExplainer` is one of the explanation methods offered by the [`shap`](https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html) package. Similarly to `h-shap`, `PartitionExplainer` explores coalitions of players as an approximation to the [Owen value](https://link.springer.com/article/10.1007/s10100-009-0100-8). Source code is available on [GitHub](https://github.com/slundberg/shap).\n",
        "\n",
        "* `pytorch-grad-cam` is a Python package that implements [`Grad-CAM`](https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html), a gradient-based explanation method based on [Class Activation Mapping](https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html). Source code is available on [GitHub](https://github.com/jacobgil/pytorch-grad-cam).\n",
        "\n",
        "`h-shap` and `PartitionExplainer`---as other explanation methods based on game-theoretic quantities---satisfy certain desirable theoretical properties. `h-shap` is particularly useful when the task is to find a concept of interest in an image (e.g., abnormality detection). On the other hand, gradient-based methods such as `Grad-CAM` currently lack general, precise mathematical guarantees on what features they will retrieve.\n",
        "\n",
        "For more information on `h-shap`, `PartitionExplainer`, and `Grad-CAM`, refer to the papers [_\"Fast Hierarchical Games for Image Explanations\"_](https://ieeexplore.ieee.org/document/9826424), [_\"A Unified Approach to Interpreting Model Predictions\"_](https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html), and [_\"Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization\"_](https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html), respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c_s6np1rWPa"
      },
      "outputs": [],
      "source": [
        "# Install h-shap\n",
        "!python -m pip install h-shap --upgrade\n",
        "\n",
        "# Install shap\n",
        "!python -m pip install shap --upgrade\n",
        "\n",
        "# Install pytorch-grad-cam\n",
        "!python -m pip install grad-cam --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeAm6EOqrWPb"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "from hshap import Explainer\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from skimage.filters import threshold_otsu\n",
        "from time import time\n",
        "\n",
        "\n",
        "def hshap_explain(hexp: Explainer, x: torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    A function that uses h-shap to explain a model's prediction.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    hexp: Explainer\n",
        "        The h-shap explainer to use.\n",
        "    x: torch.Tensor\n",
        "        The input to the model.\n",
        "    \"\"\"\n",
        "    s = 64\n",
        "    R = np.linspace(0, s, 4, endpoint=False)\n",
        "    A = np.linspace(0, 2 * np.pi, 8, endpoint=False)\n",
        "\n",
        "    explanation = hexp.cycle_explain(\n",
        "        x=x,\n",
        "        label=0,\n",
        "        s=s,\n",
        "        R=R,\n",
        "        A=A,\n",
        "        threshold_mode=\"absolute\",\n",
        "        threshold=0.0,\n",
        "        softmax_activation=False,\n",
        "        batch_size=2,\n",
        "        binary_map=False,\n",
        "    )\n",
        "    return explanation.squeeze().numpy()\n",
        "\n",
        "\n",
        "def gradcam_explain(cam: GradCAM, x: torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    A function that uses grad-cam to explain a model's prediction.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    cam: GradCAM\n",
        "        The grad-cam explainer to use.\n",
        "    x: torch.Tensor\n",
        "        The input to the model.\n",
        "    \"\"\"\n",
        "    torch.set_grad_enabled(True)\n",
        "    x = x.unsqueeze(0)\n",
        "    explanation = cam(input_tensor=x)\n",
        "    torch.set_grad_enabled(False)\n",
        "    return explanation.squeeze()\n",
        "\n",
        "\n",
        "def partexp_f(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    A helper function to initialize the\n",
        "    PartitionExplainer instance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    x: np.ndarray\n",
        "        The input to the model.\n",
        "    \"\"\"\n",
        "    x = torch.tensor(x).float()\n",
        "    x = x.permute(0, 3, 1, 2)\n",
        "    x = x.to(device)\n",
        "    return model(x).detach().cpu().numpy()[..., -1]\n",
        "\n",
        "\n",
        "def partexp_explain(partexp: shap.PartitionExplainer, x: torch.tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    A function that uses PartitionExplainer to explain a model's prediction.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    partexp: PartitionExplainer\n",
        "        The PartitionExplainer explainer to use.\n",
        "    x: np.ndarray\n",
        "        The input to the model.\n",
        "    \"\"\"\n",
        "    mav_evals = 128\n",
        "\n",
        "    x = x.permute(1, 2, 0).cpu().numpy()\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    explanation = partexp(x, max_evals=128, fixed_context=0)\n",
        "    return explanation.values[0].sum(axis=-1)\n",
        "\n",
        "\n",
        "def threshold_explanation(explanation):\n",
        "    \"\"\"\n",
        "    A helper function that thresholds an\n",
        "    explanation using Otsu's method.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    explanation: np.ndarray\n",
        "        The explanation to threshold.\n",
        "    \"\"\"\n",
        "    _t = threshold_otsu(explanation.flatten())\n",
        "    explanation = explanation * (explanation > _t)\n",
        "    abs_values = np.abs(explanation.flatten())\n",
        "    _max = np.nanpercentile(abs_values, 99)\n",
        "    return explanation, _max\n",
        "\n",
        "\n",
        "# Load the reference value used\n",
        "# to mask features\n",
        "reference = torch.load(download(f\"{base_url}/reference.pt?raw=true\"))\n",
        "reference = reference.to(device)\n",
        "\n",
        "# Initialize h-shap\n",
        "hexp = Explainer(model=model, background=reference)\n",
        "\n",
        "# Initialize PartitionExplainer\n",
        "reference = reference.permute(1, 2, 0)\n",
        "reference = reference.cpu().numpy()\n",
        "masker = shap.maskers.Image(reference)\n",
        "partexp = shap.Explainer(partexp_f, masker)\n",
        "\n",
        "# Initialize Grad-CAM\n",
        "cam = GradCAM(model=model, target_layers=[model.encoder.layer4[-1]], use_cuda=True)\n",
        "\n",
        "explainers = [\n",
        "    [\"h-Shap\", hexp, hshap_explain],\n",
        "    [\"PartitionExplainer\", partexp, partexp_explain],\n",
        "    [\"Grad-CAM\", cam, gradcam_explain],\n",
        "]\n",
        "fig, axes = plt.subplots(len(explainers), len(images), figsize=(16, 9))\n",
        "for i in range(len(images)):\n",
        "    # Preprocess image for explanation\n",
        "    x = images[i]\n",
        "    x = tf.to_tensor(x)\n",
        "    x = x.repeat(3, 1, 1)\n",
        "    x = tf.normalize(x, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    x = x.to(device)\n",
        "\n",
        "    for j, (explainer_name, explainer, explain) in enumerate(explainers):\n",
        "        print(f\"Explaining image {i+1} with {explainer_name} ...\", end=\" \")\n",
        "        t0 = time()\n",
        "\n",
        "        # Explain the model prediction on the image\n",
        "        explanation = explain(explainer, x)\n",
        "        # Threshold the explanation to reduce noise\n",
        "        explanation, _max = threshold_explanation(explanation)\n",
        "        print(f\"done in {time() - t0:.2f} seconds\")\n",
        "\n",
        "        # Visualize the image, ground-truth, and explanation\n",
        "        ax = axes[j, i]\n",
        "        ax.imshow(images[i], cmap=\"gray\")\n",
        "        ax.imshow(explanation, cmap=\"bwr\", vmin=-_max, vmax=_max, alpha=0.5)\n",
        "\n",
        "        annotation_df = gt[gt[\"SOPInstanceUID\"] == sop_ids[i]]\n",
        "        annotate(annotation_df, ax)\n",
        "\n",
        "        ax.set_title(explainer_name)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlH0mgHbrWPd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 ('cuda102')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd8c972ecb72e669b05b9af0bbaad01a2103da39053b5ad2ceb924e75319f022"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}